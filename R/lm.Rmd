---
title: "Recipe: Generalized Linear Model"
subtitle: "Multiple Linear Regression"
author: "Generated by the R-package [`dessert`](https://github.com/gerkovink/dessert)"
output: 
   bookdown::html_document2:
      toc: true
      toc_depth: 3
      toc_float:
         collapsed: true
         smooth_scroll: true
      number_sections: true
   bookdown::word_document2:
      toc: true
      toc_depth: 3
      number_sections: true
   bookdown::pdf_document2:
      toc: true
      toc_depth: 3
      number_sections: true
---

```{r}
#model <- lm(Sepal.Length ~ ., data = iris)
```


# Model Summary

```{r}
summary <- summary(model)
summary
```

# Model Assumptions

## Linearity

The linearity assumption holds if there is a linear relationship between the independent variable(s), *`r paste(gsub(":", "*", labels(summary[["terms"]])), collapse = ", ")`*, and the dependent variable, *`r deparse(summary[["terms"]][[2]])`*.
 
If, in fact, the relationship in the data is non-linear, then the predictions are likely to be seriously in error. This error will become especially problematic when you extrapolate beyond sample data.
 
You can test the linearity assumption through visual inspection. Figure \@ref(fig:lm-linearity) shows the fitted values (predicted responses for *`r deparse(summary[["terms"]][[2]])`*) on the *x-axis* and corresponding residuals of those predictions on the *y-axis*.

```{r lm-linearity, echo=FALSE, fig.cap="*Residuals versus fitted values plot*"}
plot(model, which = 1, id.n = 0)
```

There is support for the linearity assumption if there is no pattern between the residuals and fitted values. That is the red line should be approximately a horizontal line centered around zero. A deviation from a straight line may suggest that is a problem with the linearity assumption. 

If there is a violation of the linearity assumption, you could transform variables. For example, you could apply the following transformations $log(x)$, $\sqrt{x}$, and $x^2$ on the outcome variable.

> **Example** *Interpretation of residuals versus fitted plots*
> 
> ![](`r paste(figure_loc, "lm-linearity.png", sep = "/")`)
> 
> <small>A) The red line follows a straight line around zero. Therefore, we can assume that there is a linear relationship between the predictors and outcome.</small>
> <small>B) The red line strongly deviates from a straight line. So, we cannot assume linearity.</small>
> <small>C) In this case, the red line also deviates from a straight line. Hence, we cannot assume linearity</small>

```{r eval=FALSE, fig.height=3, fig.width=9, include=FALSE}
png(file="lm-linearity.png", width=1500, height=500, res=200)
par(mfrow=c(1,3))
plot(lm(x           ~ I(x + e), data = example), which = 1, id.n = 0)
title(outer=TRUE, adj = 0,   main="A", cex.main=2, col="black", line=-2)
plot(lm(I(x^2)      ~ I(x + e), data = example), which = 1, id.n = 0)
title(outer=TRUE, adj = 1/3, main="B", cex.main=2, col="black", line=-2)
plot(lm(I(log10(x)) ~ I(x + e), data = example), which = 1, id.n = 0)
title(outer=TRUE, adj = 2/3, main="C", cex.main=2, col="black", line=-2)
dev.off()
```

---

## Normality of Residuals

The residuals of the regression should follow a normal distribution with a mean of zero. 

Violations of normality create problems in determining whether model coefficients are significant and in calculating confidence intervals.

You can visually check the assumption using the Q-Q (quantile-quantile) plot. A Q–Q plot is a graphical method for comparing two probability distributions. Figure \@ref(fig:lm-normality) compares the standardized residuals with the theoretical quantiles from a normal distribution. 

```{r lm-normality, echo=FALSE, fig.cap="*Q-Q plot of the standardized residuals*"}
plot(model, which = 2, id.n = 0)
```

If the residuals fall along a approximately straight line at a 45-degree angle, then you can assume the residuals are normally distributed.

If there is a violation of the normality assumption, you could transform variables. For example, you could consider applying the following transformations $log(x)$, $\sqrt{x}$, and $x^2$.

> **Example** *Interpretation of Q-Q plot*
>
> ![](`r paste(figure_loc, "lm-normality.png", sep = "/")`)
> 
> <small>A) The residuals fall along a approximately straight line at a 45-degree angle. Therefore, we can assume that the normality assumption holds.</small>
> <small>B) At the tails the residuals somewhat deviate from the straight line. Still, some small violations may have little practical effect on the analysis.</small>
> <small>C) In this case, the residuals strongly deviate from the straight line. Hence, we should not assume normality.</small>

```{r eval=FALSE, fig.height=3, fig.width=9, include=FALSE}
png(file="lm-normality.png", width=1500, height=500, res=200)
par(mfrow=c(1,3))
plot(lm(x ~ I(x + e),                    data = example), which = 2, id.n = 0)
title(outer=TRUE, adj = 0,   main="A", cex.main=2, col="black", line=-2)
plot(lm(x ~ I(x + rbeta(500, 2.0, 2.0)), data = example), which = 2, id.n = 0)
title(outer=TRUE, adj = 1/3, main="B", cex.main=2, col="black", line=-2)
plot(lm(x ~ I(x + rbeta(500, 0.5, 0.5)), data = example), which = 2, id.n = 0)
title(outer=TRUE, adj = 2/3, main="C", cex.main=2, col="black", line=-2)
dev.off()
```

---

## Homoscedasticity

The assumption of homoscedasticity holds if the variance of the residuals is similar across the values of the independent variables. 

If there is a presence of heteroscedasticity, the regression estimators and predictions remain unbiased and consistent. However, the estimations are inefficient. 

You can visually find support for the assumption using the scale-location plot. Figure \@ref(fig:lm-homoscedasticity) shows if residuals are spread equally along the fitted values. 

```{r lm-homoscedasticity, echo=FALSE, fig.cap="*Scale-location plot*"}
plot(model, which = 3, id.n = 0)
```

There is support for the homoscedasticity assumption if you see a horizontal line with equally spread points.

In the case of heteroscedasticity, you can transform the variables. For example, you could consider applying the following transformations $log(x)$, $\sqrt{x}$, and $x^2$.

> **Example** *Interpretation of Q-Q plot*
> 
> ![](`r paste(figure_loc, "lm-homoscedasticity.png", sep = "/")`)
> 
> <small>A) The red line resembles a straight line. Hence, we can assume homoscedasticity.</small>
> <small>B) The variance of the standardized residuals somewhat increases as the fitted values increase. This plot indicates a moderate violation of the homoscedasticity assumption.</small>
> <small>C) In this case, the variance of the standardized residuals differs strongly between the fitted values. Therefore, there is heteroscedasticity of residuals.</small>

```{r eval=FALSE, fig.height=3, fig.width=9, include=FALSE}
png(file="lm-homoscedasticity.png", width=1500, height=500, res=200)
par(mfrow=c(1,3))
plot(lm(I(n + e)                  ~ n, data = example), which = 3, id.n = 0)
title(outer=TRUE, adj = 0,   main="A", cex.main=2, col="black", line=-2)
plot(lm(I(x + rnorm(500, sd = x)) ~ x, data = example), which = 3, id.n = 0)
title(outer=TRUE, adj = 1/3, main="B", cex.main=2, col="black", line=-2)
plot(lm(I(n + rnorm(500, sd = n)) ~ n, data = example), which = 3, id.n = 0)
title(outer=TRUE, adj = 2/3, main="C", cex.main=2, col="black", line=-2)
dev.off()
```

---

## Independence of Observations

The assumption independence of the observations means that observations are not related to one another or clustered. 
Therefore, the residuals should also not be correlated. Otherwise, one observation provides information about another observation.  

A violation of this assumption will cause the standard errors to be inflated. 

You can visually check the independence assumption using an autocorrelation plot of the residuals. Autocorrelation measures the degree of similarity between the residuals and a lagged version of itself over the given range of time intervals. Figure \@ref(fig:lm-independence) shows the autocorrelation of the residuals. The *x-axis* corresponds to the different lags of the residuals. Whereas the *y-axis* shows the correlation of each lag. The dashed blue line represents the significance level.

```{r lm-independence, echo=FALSE, fig.cap="*Autocorrelation of the residuals plot*"}
acf(model$residuals, type = "correlation")
```

The first vertical bar shows the correlation of a residual with itself and therefore is always one. If there is independence of observations, the subsequent vertical bars would drop quickly to almost zero or at least between the dashed blue lines.

If the autocorrelation remains strong at higher lag values, it might be good to consider if there is some form of clustering the data and if another statistical method may be more appropriate to deal with this type of clustering.

> **Example** *Interpretation of autocorrelation plot*
> 
> ![](`r paste(figure_loc, "lm-independence.png", sep = "/")`)
> 
> <small>A) For all lag values (except 1) there is a small autocorrelation. This result supports that the observations are independent</small>
> <small>B) For lag values less than 20, there still remains some autocorrelation. This plot may indicate that are some issues with the independence assumption.</small>
> <small>C) This plot shows a strong autocorrelation for each lag value. The high autocorrelation signals the independence assumption is violated.</small>

```{r eval=FALSE, fig.height=3, fig.width=9, include=FALSE}
png(file="lm-independence.png", width=1500, height=500, res=200)
par(mfrow=c(1,3))

p1 <- acf(example$e,                               type = "correlation", lag.max = 50, plot = F)
plot(p1, main = "Example 1")
title(outer=TRUE, adj = 0,   main="A", cex.main=2, col="black", line=-2)

p2 <- acf(exp(-0.1  * example$n) * sin(example$n), type = "correlation", lag.max = 50, plot = F)
plot(p2, main = "Example 2")
title(outer=TRUE, adj = 1/3, main="B", cex.main=2, col="black", line=-2)

p3 <- acf(exp(-0.01 * example$n) * sin(example$n), type = "correlation", lag.max = 50, plot = F)
plot(p3, main = "Example 3")
title(outer=TRUE, adj = 2/3, main="C", cex.main=2, col="black", line=-2)

dev.off()
```

---

## Multicollinearity 

Linear regression assumes no multicollinearity. This assumption implies that the independent variables are not strongly correlated. 

The presence of multicollinearity reduces the statistical significance of the independent variables.

A variance inflation factor (VIF) provides a measure of multicollinearity. Figure \@ref(fig:lm-multicollinearity) shows the VIF for each predictor. 

```{r lm-multicollinearity, echo=FALSE, fig.cap="*Variance inflation factor of each predictor*"}
barplot(
   sort(car::vif(model)), 
   main = "Variance Inflation Factor (VIF)", 
   xlab = "VIF",
   ylab = "Independent variables", 
   col = "white",
   las = 1,
   horiz = T
)

abline(v = 4,  lwd = 1, lty = 2)
abline(v = 10, lwd = 1, lty = 1)
```

A large VIF for an independent variable indicates a highly collinear relationship to the other variables. As a general rule of thumb, a VIF value greater than 4 suggests further investigation, and VIFs exceeding 10 are indications of serious multicollinearity requiring correction.

In the case of multicollinearity, you should consider adjusting the structure of the model and the selection of independent variables.

---

## No Influential Outliers

Influential points are outliers with high leverage, which strongly affects the slope of the regression estimates. 

An outlier is a point whose *`r deparse(summary[["terms"]][[2]])`* value does not follow the general trend of the rest of the data. A data point has high leverage if it has extreme for one of the predictor (*`r paste(gsub(":", "*", labels(summary[["terms"]])), collapse = ", ")`*) values. See the example below for a visual distinction between the two types of unusual observations. 

If present, these points can bias parameter estimates and standard errors.

The metric Cook’s distance helps to determine the influence of a value. It defines influence as a combination of leverage and residual size. A rule of thumb is that an observation has high influence if Cook’s distance exceeds $4/(n - p - 1)$, where $n$ is the number of observations and $p$ the number of predictor variables.

```{r lm-cooks, echo=FALSE, fig.cap="*Variance inflation factor of each predictor*"}
plot(model, 4, id.n = 0)
abline(
   h   = 4/(nrow(model[["model"]]) - ncol(model[["model"]])), 
   lwd = 1, 
   lty = 2
)
```

Figure \@ref(fig:lm-cooks) shows the cooks distance for each observation. 

Another plot to help identify influential points is the *residual versus leverage* plot. The *x-axis* represents the leverage. The *y-axis* denotes the standardized residuals. The influential points are generally located at the upper right and lower right corners. Those regions contain observations that are outliers with high leverage. Figure \@ref(fig:lm-leverage) shows the *residual versus leverage*. 

```{r lm-leverage, echo=FALSE, fig.cap="*Residuals versus leverage plot*"}
plot(model, which = 5, id.n = 0)
```

> **Example** *Interpretation of residuals versus leverage plot*
> 
> ![](`r paste(figure_loc, "lm-leverage.png", sep = "/")`)
> 
> <small>A) This plot shows a point which has high leverage, but is not an outlier, therefore is not influential</small>
> <small>B) This plot shows a point which is an outlier with low leverage, hence not influential</small>
> <small>C) This plot shows a point which is an outlier with high leverage, therefore is an influential point</small>

```{r eval=FALSE, fig.height=9, fig.width=6, include=FALSE}
png(file="lm-leverage.png", width=1000, height=1500, res=200)

par(mfrow=c(3,2))

data <- tibble(x = rnorm(100), e = rnorm(100, sd = 0.3)) %>%
   mutate(y = x, c = "black")

data[100,] <- tibble(x = 7, e = 0, y = 7, c = "red")
plot(data$x, data$y + data$e, col=data$c, xlab="x", ylab="y")
fit <- lm(I(y + e) ~ x, data = data)
abline(fit)
plot(fit, which = 5, id.n = 1, col=data$c)
title(outer = TRUE, adj = 0, main="A", cex.main=2, line=-2)

data[100,] <- tibble(x = 0, e = 0, y = 7, col = "red")
plot(data$x, data$y + data$e, col=data$c, xlab="x", ylab="y")
fit <- lm(I(y + e) ~ x, data = data)
abline(fit)
plot(fit, which = 5, id.n = 1, col=data$c)
title(outer = TRUE, adj = 0, main="B", cex.main=2, line=-21.5)

data[100,] <- tibble(x = 3, e = 0, y = 10, col = "red")
plot(data$x, data$y + data$e, col=data$c, xlab="x", ylab="y")
fit <- lm(I(y + e) ~ x, data = data)
abline(fit)
plot(fit, which = 5, id.n = 1, col=data$c)
title(outer = TRUE, adj = 0, main="C", cex.main=2, line=-41)

dev.off()
```

# Model Diagnostics

## Three-fold Cross-validation

```{r message=FALSE, warning=FALSE, include=FALSE}
# load packages 
library(tidymodels)
library(tidyverse)

# set model specification
lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# create model recipe 
lm_recipe <- recipe(formula = model[["terms"]], data = model[["model"]])

# create work flow
lm_workflow <- workflow() %>%
   add_recipe(lm_recipe) %>%
   add_model(lm_model) 

# apply 3 fold cross validation
lm_cv <- lm_workflow %>% 
   fit_resamples(
      resamples = vfold_cv(model[["model"]], v = 3), 
      control   = control_resamples(save_pred = TRUE),
      metrics   = metric_set(rmse, rsq, mae)
   )
```

```{r echo=FALSE}
# view model metrics
test_object <- collect_metrics(lm_cv)
test_object
```

```{r echo=FALSE}
# prediction plot
collect_predictions(lm_cv) %>% 
   arrange(.row) %>%
   mutate(
      id   = as.factor(parse_number(id)),
      pred = model[["fitted.values"]]) %>%
   `colnames<-`(c("Fold", "Pred", "Row", "Actual", "Confiq", "Pred_all")) %>%
   ggplot(aes(x = Pred_all, y = Actual, color = Fold, shape = Fold)) +
   geom_point() +
   geom_point(aes(y = Pred), size = 1, alpha = 0.5) +
   geom_smooth(method = "lm", formula = y ~ x, fill = NA) +
   theme_bw() + 
   theme(legend.position="top") +
   labs(
      x = "Predicted (fit to all data)",
      y = deparse(model[["terms"]][[2]]),
      caption = "Small symbols show cross-validation predicted values"
   )
```


## Robust Linear Regression 

Robust regression is done by iterated re-weighted least squares. 

```{r}
library(MASS)

model_robust <- rlm(model[["terms"]], data = model[["model"]])

summary(model_robust)
```

```{r}
# this is a test table
tibble(x = c("test"))
data.frame(x = c("test"))
```

```{r}
# this is another test table 
knitr::kable(tibble(x = c("test")))
```



