---
title: "Multiple Least Squares Regression"
subtitle: "Generated by the R-package:  [`dessert`](https://github.com/gerkovink/dessert) &#x1F370;"
authors: 
  - name: "G. Vink" 
    affiliations:
      - name: Utrecht University 
        city: Utrecht
        state: Utrecht
  - name: "H. I. Oberman"
  - name: "E. P. Swens"
format:  
  html:
    toc: true
    number-depth: 3
    link-external-newwindow: true
    embed-resources: true
title-block-banner: true
number-sections: true
editor: source
---

```{r echo=FALSE}
load("regression.rdata")
model <- input
summary <- summary(model)
predictors <- paste(
  paste0("<code>", labels(summary[["terms"]]), "</code>"), 
  collapse = ", "
)
outcome <- paste0("<code>", deparse(summary[["terms"]][[2]]), "</code>")
```

## Model Summary

```{r echo=FALSE}
summary
```

------------------------------------------------------------------------

## Model Assumptions

### Linearity

The linearity assumption holds if there is a linear relationship between the independent variable(s) `r predictors` and the dependent variable, `r outcome`.

A violation of this assumption can result in unreliable predictions and lead to substantial errors, particularly when extrapolating beyond the sample data.

To assess the validity of the linearity assumption, it is common practice to perform a visual inspection of the fitted values against the residuals. @fig-lm-linearity shows the fitted values (predicted responses for `r outcome`) against the corresponding residuals.

```{r echo=FALSE}
#| label: fig-lm-linearity
#| fig-cap: "Residuals versus fitted values plot" 
plot(model, which = 1, id.n = 0)
```

A horizontal line centered around zero, with little to no deviation, would indicate support for the linearity assumption. However, if a clear pattern is present, it may suggest a violation of the linearity assumption.

::: callout-tip
## Tip

In the case of a violation of the linearity assumption, one possible solution is to transform the variables in the model. For instance, applying transformations such as $log(x)$, $\sqrt{x}$, and $x^2$ on `r outcome` can help to address any linearity issues and improve the fit of the model.
:::

::: callout-note
## Example

![](regression/lm-linearity.png)

A)  The residual plot exhibits a straight line centered around zero, suggesting that there is a linear relationship between the predictors and outcome.
B)  The residual plot shows a pronounced deviation from a straight line, indicating that linearity cannot be assumed.
C)  The residual plot similarly displays a deviation from a straight line, implying that linearity is not present.
:::

------------------------------------------------------------------------

### Normality of Residuals

The residuals of the regression should follow a normal distribution with a mean of zero. Deviations from normality can create problems in the interpretation of model coefficients and affect the calculation of confidence intervals.

To assess this assumption, a Q-Q (quantile-quantile) plot can be utilized. A Q--Q plot is a graphical method for comparing two probability distributions. @fig-lm-normality compares the standardized residuals of the predicted values of `r outcome` with the theoretical quantiles from a normal distribution.

```{r echo=FALSE}
#| label: fig-lm-normality
#| fig-cap: Q-Q plot of the standardized residuals
plot(model, which = 2, id.n = 0)
```

If the residuals form a line with a slope of approximately 45 degrees, the normality assumption holds.

::: callout-tip
## Tip

If there is a violation of the normality assumption, you could consider transforming variables. For instance, applying transformations such as $log(x)$, $\sqrt{x}$, and $x^2$ on `r outcome` can help to address any normality issues and improve the fit of the model.
:::

::: callout-note
## Example

![](regression/lm-normality.png)

A)  The residuals being approximately straight at a 45-degree angle is a good indication that the normality assumption holds.
B)  Deviations from the straight line at the tails may suggest that the normality assumption is not perfectly met, but small deviations may not significantly impact the analysis.
C)  Strong deviation from the straight line in the residuals indicates that the normality assumption is not met and should not be assumed.
:::

------------------------------------------------------------------------

### Homoscedasticity

The assumption of homoscedasticity holds if the variance of the residuals is similar across the values of the independent variables.

In the presence of heteroscedasticity, the regression predictions and estimators are still unbiased and consistent, however the estimations are inefficient.

One way to assess the homoscedasticity assumption is through the scale-location plot, which shows if the residuals are evenly spread out along the fitted values. @fig-lm-homoscedasticity shows if residuals are spread equally along the fitted values.

```{r echo=FALSE}
#| label: fig-lm-homoscedasticity
#| fig-cap: "Scale-location plot" 
plot(model, which = 3, id.n = 0)
```

A line that is horizontally aligned with equal spacing between the points supports the homoscedasticity assumption.

::: callout-tip
## Tip

If heteroscedasticity is detected, transforming the variables can help resolve the issue. For example, you could consider applying the following transformations $log(x)$, $\sqrt{x}$, and $x^2$ on `r outcome`.
:::

::: callout-note
## Example

![](regression/lm-homoscedasticity.png)

A)  The red line resembling a straight line is a good indication that the homoscedasticity assumption holds.
B)  The increase in the variance of standardized residuals as the fitted values increase suggests a moderate violation of the homoscedasticity assumption.
C)  Strong differences in the variance of standardized residuals between different fitted values indicate heteroscedasticity of residuals.
:::

------------------------------------------------------------------------

### Independence of Observations

The assumption independence of the observations means that observations are not related to one another or clustered. As a consequence, the residuals should not be correlated. Otherwise, one observation provides information about another observation.

If the independence assumption is violated, the standard errors of the regression analysis are inflated.

One way to check for the independence assumption is through an autocorrelation plot of residuals. @fig-lm-independence shows the autocorrelation of the residuals. The autocorrelation plot shows the similarity between residuals and a lagged version of itself over a range of time intervals. The x-axis represents different lags of the residuals and the y-axis shows the correlation between each lag. The significance level is represented by a dashed blue line.

```{r echo=FALSE}
#| label: fig-lm-independence
#| fig-cap: "Autocorrelation of the residuals plot" 
stats::acf(model$residuals, type = "correlation")
```

The first vertical bar (i.e., lag-0) represents the correlation of a residual with itself and is always one. In the absence of autocorrelation, the subsequent vertical bars should decrease quickly to zero or be between the significance level lines.

::: callout-tip
## Tip

If autocorrelation remains strong at higher lag values, it may indicate clustering in the data, and it may be necessary to consider using a different statistical method.
:::

::: callout-note
## Example

![](regression/lm-independence.png)

A)  Autocorrelation is small for all lag values except 1, which supports the independence of observations.
B)  Autocorrelation still exists for lag values less than 20, suggesting potential problems with the independence assumption.
C)  High autocorrelation at all lag values is indicated by this plot, violating the independence assumption.
:::

### Multicollinearity

Linear regression assumes no multicollinearity. This assumption implies that the independent variables are not strongly correlated.

The presence of multicollinearity reduces the statistical significance of the independent variables.

A variance inflation factor (VIF) provides a measure of multicollinearity. @fig-lm-multicollinearity shows the VIF for each predictor.

```{r echo=FALSE}
#| label: fig-lm-multicollinearity
#| fig-cap: "Variance inflation factor of each predictor" 
barplot(
   sort(car::vif(model)), 
   main = "Variance Inflation Factor (VIF)", 
   xlab = "VIF",
   ylab = "Independent variables", 
   col = "white",
   las = 1,
   horiz = T
)

abline(v = 4,  lwd = 1, lty = 2)
abline(v = 10, lwd = 1, lty = 1)
```

A large VIF for an independent variable indicates a highly collinear relationship to the other variables. As a general rule of thumb, a VIF value greater than 4 suggests further investigation, and VIFs exceeding 10 are indications of extreme multicollinearity.

::: callout-tip
## Tip

In the case of extreme multicollinearity, you could consider adjusting the structure of the model and the selection of independent variables.
:::

### Influential Points

Technically no influential points is not an assumption, but it is good practice to investigate the presence of influential points. Because these points can bias parameter estimates and yield bad predictions.

Cook's distance is commonly used estimate of the influence of a data point when performing a least-squares regression analysis. A rule of thumb is that an observation has high influence if Cook's distance exceeds $4/(n - p - 1)$, where $n$ is the number of observations and $p$ the number of predictor variables.

One way to identify influential points is through the residuals vs leverage plot. Note, that an influential point is an outlier with high leverage. First, an outlier is a data point whose response of `r outcome` does not follow the general trend of the rest of the data. Second, a data point has high leverage if it has extreme predictor values.

```{r echo=FALSE}
#| label: fig-lm-leverage
#| fig-cap: "Residuals versus leverage plot" 
plot(model, which = 5, id.n = 0)
```

@fig-lm-leverage shows the residuals versus leverage. On this plot, influential points are located at the upper right corner or at the lower right corner.

::: callout-tip
## Tip

If there are influential points in your dataset, we recommend that you examine the data and investigate these points. Additionally, we suggest using a robust linear regression model, see @sec-robust-linear-regression.
:::

::: callout-note
## Example

![](regression/lm-leverage-1.png)

A)  This plot shows a point which has high leverage, but is not an outlier, therefore is not influential.

![](regression/lm-leverage-2.png)

B)  This plot shows a point which is an outlier with low leverage, hence not influential.

![](regression/lm-leverage-3.png)

C)  This plot shows a point which is an outlier with high leverage, therefore is an influential point.
:::

## Model Diagnostics

### Three-fold Cross-validation

Three-fold cross-validation is a technique for evaluating the performance of a linear regression model by dividing the available data into three subsets, or folds, and using each fold in turn to evaluate the model that was trained on the other two folds. The advantage of three-fold cross-validation is that it provides a more accurate estimate of the model's performance on new data than a single train-test split, especially when the dataset is small. It also helps to detect overfitting, which occurs when the model fits the training data too closely and does not generalize well to new data.

```{r message=FALSE, warning=FALSE, include=FALSE}
# load packages 
library(tidymodels)
library(tidyverse)

# set model specification
lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# create model recipe 
lm_recipe <- recipe(formula = model[["terms"]], data = model[["model"]])

# create work flow
lm_workflow <- workflow() %>%
   add_recipe(lm_recipe) %>%
   add_model(lm_model) 

# apply 3 fold cross validation
lm_cv <- lm_workflow %>% 
   fit_resamples(
      resamples = vfold_cv(model[["model"]], v = 3), 
      control   = control_resamples(save_pred = TRUE),
      metrics   = metric_set(rmse, rsq, mae)
   )
```

```{r echo=FALSE}
# view model metrics
collect_metrics(lm_cv) %>% 
  mutate(
    metric = 
    c("Mean absolute error", "Root-mean-square error", "R-Squared"),
    estimate = round(mean, 3), 
    se = round(std_err, 3)) %>%
  select(metric, estimate, se) %>%
  knitr::kable(
    col.names = c("Metric", "Estimate", "S.E.")
  )
```

```{r echo=FALSE}
# prediction plot
collect_predictions(lm_cv) %>% 
  arrange(.row) %>%
  mutate(
    id   = as.factor(parse_number(id)),
    pred = model[["fitted.values"]]) %>%
    `colnames<-`(c("Fold", "Pred", "Row", "Actual", "Confiq", "Pred_all")) %>%
  ggplot(aes(x = Pred_all, y = Actual, color = Fold, shape = Fold)) +
  geom_point() +
  geom_point(aes(y = Pred), size = 1, alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ x, fill = NA) +
  theme_bw() + 
  theme(legend.position="top") +
  labs(
    x = "Predicted (fit to all data)",
    y = deparse(model[["terms"]][[2]]),
    caption = "Small symbols show cross-validation predicted values"
  )
```

### Robust Linear Regression {#sec-robust-linear-regression}

Robust linear regression is a statistical method used for fitting a linear model to data in the presence of outliers or influential observations. It seeks to minimize the influence of extreme values on the estimation of regression coefficients. Robust linear regression can be used to obtain more reliable estimates of regression coefficients and make more accurate predictions on new data. You should run a robust linear regression when you suspect that your data may contain outliers or influential observations that could affect the accuracy of your model.

```{r echo=FALSE}
formula <- model[["terms"]]
data    <- model[["model"]]
model_robust <- MASS::rlm(formula, data = data)

summary(model_robust)
```
